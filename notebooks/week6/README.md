# Week 6: Production Monitoring and Caching with Langfuse and Redis

## Overview

Week 6 adds production-grade monitoring and intelligent caching to our RAG system. We integrate Langfuse for complete pipeline observability and Redis for high-performance response caching.

## What We Built

- **Langfuse Integration**: End-to-end RAG pipeline tracing and analytics
- **Redis Caching**: 150-400x faster responses for repeated queries
- **Performance Monitoring**: Real-time metrics and system health
- **Production Ready**: Enterprise-grade observability and optimization

## Architecture

<p align="center">
  <img src="../../static/week6_monitoring_and_caching.png" alt="Week 6 Monitoring & Caching Architecture" width="900">
  <br>
  <em>Week 6 architecture with Langfuse tracing and Redis caching integration</em>
</p>

### Data Flow
```
Query → Cache Check → [Hit: ~100ms] | [Miss: Full Pipeline ~15s] → Cache Store → Langfuse Trace
```

## Key Features

### **Langfuse Observability**
- Complete RAG pipeline tracing with performance breakdowns
- User analytics, query patterns, and success rate tracking
- Real-time monitoring dashboard with cost and usage metrics
- Quality insights with answer relevance and source attribution

### **Redis Intelligent Caching**
- **Exact-Match Strategy**: Parameter-aware cache keys for precise matching
- **Performance**: 150-400x faster responses for repeated queries (~100ms vs 15-20s)
- **TTL Management**: 24-hour default expiration with configurable settings
- **Future Enhancement**: Can be upgraded to semantic similarity caching for fuzzy matching

## Quick Start

### Environment Setup
```bash
# Required environment variables
LANGFUSE__SECRET_KEY=sk_lf_your_secret_key
LANGFUSE__PUBLIC_KEY=pk_lf_your_public_key
REDIS__HOST=redis
REDIS__TTL_HOURS=24
```

### Start Services
```bash
docker compose up --build -d
```

### Test Caching Performance
```bash
# First request (cache miss ~15-20s)
curl -X POST "http://localhost:8000/api/v1/ask" \
  -H "Content-Type: application/json" \
  -d '{"query": "What are transformers?", "top_k": 3}'

# Second identical request (cache hit ~100ms)
curl -X POST "http://localhost:8000/api/v1/ask" \
  -H "Content-Type: application/json" \
  -d '{"query": "What are transformers?", "top_k": 3}'
```

## Performance Benchmarks

| Scenario | Response Time | Improvement |
|----------|---------------|-------------|
| **Cache Miss** | 15-20 seconds | Baseline |
| **Cache Hit** | 50-100ms | **150-400x faster** |
| **Monitoring Overhead** | <2% | Negligible impact |

## Testing

### Run the Notebook
```bash
jupyter notebook notebooks/week6/week6_cache_testing.ipynb
```

### Monitor System Health
```bash
# Check Redis connectivity
redis-cli ping

# View cache statistics  
curl "http://localhost:8000/api/v1/health"

# Access Langfuse dashboard
# Visit: https://cloud.langfuse.com (or your self-hosted instance)
```

## Troubleshooting

| Issue | Solution |
|-------|----------|
| **Cache not working** | Check Redis: `redis-cli ping` |
| **No Langfuse traces** | Verify environment variables: `LANGFUSE__*` |
| **Slow responses** | Monitor cache hit rate and system resources |

## Next Steps

- **Enhanced Caching**: Upgrade to semantic similarity caching for fuzzy matching
- **Advanced Analytics**: Custom dashboards and A/B testing frameworks  
- **Production Scaling**: Distributed caching and automated monitoring
- **Quality Optimization**: User feedback integration and answer scoring

## Resources

- **Notebook**: [week6_cache_testing.ipynb](./week6_cache_testing.ipynb)
- **Langfuse Dashboard**: https://cloud.langfuse.com
- **Redis Documentation**: https://redis.io/docs

---

Week 6 transforms your RAG system into a production-grade service with 150-400x performance improvements and comprehensive observability.